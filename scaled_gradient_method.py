import numpy as np
import matplotlib.pyplot as plt

def scaled_gradient_method(A, b, x0, epsilon, D):
    '''
    Since the gradient method is sensitive to the condition number of the matrix A,
    we can use the scaled gradient method to improve the convergence rate.

    A - positive definite matrix associated with the objective function
    b - vector associated with the linear part objective function
    x0 - initial point
    epsilon - tolerance
    D - diagonal matrix used to scale the gradient

    x - optimal solution
    k - number of iterations
    xk - sequence of points generated by the gradient method
    '''

    # Initialization
    xk = [x0]
    x = x0
    k = 0
    grad = 2*np.dot(A, x) + b
    grad_norm = np.linalg.norm(grad)

    # Gradient method
    while grad_norm > epsilon:
        k += 1
        step_size = (np.dot(np.dot(grad.T, D), grad))/(2*np.dot(np.dot(np.dot(grad.T, D), A), np.dot(D, grad)))
        x = x - step_size*np.dot(D, grad)
        xk.append(x)
        grad = 2*np.dot(A, x) + b
        grad_norm = np.linalg.norm(grad)

    return x, k, xk


# plot the quadratic function
A = np.array([[1000, 20], [20, 1]])
b = np.array([0, 0])
x0 = np.array([1, 1000])
epsilon = 1e-5

# compute the diagonal matrix D
D = np.array([[1/1000, 0], [0, 1]])

x, k, xk = scaled_gradient_method(A, b, x0, epsilon, D)
print('The optimal solution is', int(x[0]), int(x[1]))
print('The number of iterations is', k)

# plot the quadratic function
x = np.linspace(-1000, 1000, 1000)
y = np.linspace(-1000, 1000, 1000)
X, Y = np.meshgrid(x, y)
F = 1000*X**2 + 2*X*Y + Y**2
plt.contour(X, Y, F, 20)

# plot the sequence of points generated by the gradient method
xk = np.array(xk)
plt.plot(xk[:, 0], xk[:, 1], 'ro-')
plt.show()
