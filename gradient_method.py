import matplotlib.pyplot as plt
import numpy as np

def gradient_method_quadratic(A, b, x0, epsilon):
    '''
    The gradient descent method for minimizing a quadratic function
    f(x) = 1/2*x^T*A*x + b^T*x
    g(x) = A*x + b
    
    A - positive definite matrix associated with the objective function
    b - vector associated with the linear part objective function
    x0 - initial point
    epsilon - tolerance

    x - optimal solution
    k - number of iterations
    xk - sequence of points generated by the gradient method
    '''

    # Initialization
    xk = [x0]
    x = x0
    k = 0
    grad = 2*np.dot(A, x) + b
    grad_norm = np.linalg.norm(grad)

    # Gradient method
    while grad_norm > epsilon:
        k += 1
        step_size = (grad_norm**2)/(2*np.dot(np.dot(A, grad), grad))
        x = x - step_size*grad
        xk.append(x)
        grad = 2*np.dot(A, x) + b
        grad_norm = np.linalg.norm(grad)

    return x, k, xk

# plot the quadratic function
A = np.array([[1, 0], [0, 2]])
b = np.array([0, 0])
x0 = np.array([2, 1])
epsilon = 1e-6

x, k, xk = gradient_method_quadratic(A, b, x0, epsilon)
print('The optimal solution is', int(x[0]), int(x[1]))
print('The number of iterations is', k)

# plot the quadratic function
x = np.linspace(-2, 2, 100)
y = np.linspace(-2, 2, 100)
X, Y = np.meshgrid(x, y)
F = X**2 + 2*Y**2
plt.contour(X, Y, F, 20)

# plot the sequence of points generated by the gradient method
xk = np.array(xk)
plt.plot(xk[:, 0], xk[:, 1], 'ro-')
plt.show()



